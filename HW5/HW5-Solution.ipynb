{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowed libraries: sklearn, matplotlib, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1- [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the .csv file into a pandas dataframe, it contains a list of 1200 bitstrings which each have a length of 16 bits. Call the dataset $S$. We call two bitstrings $A$ and $B$ equivalent ($A\\sim B$) if you can flip one bit at a time starting from $A$ to produce a sequence of strings $A,s_1, s_2, ..., s_n, B\\in S$ that are all within the dataset to get the string $B$. Through this notion of equivalence, we may define an equivalence relation on this set of bit strings. Using agglomerative clustering with a tolerance on distance for early stopping, we can calculate the number of equivalence classes by counting the number of clusters. In order to do this, which linkage rule should be used (single-linkage, complete-linkage, or average-linkage), which distance function should be used (Euclidean distance, Manhattan distance, or cosine distance), and what should the threshold distance be? Explain why you would pick these parameters.\n",
    "\n",
    "Hints:\n",
    "- An example of an equivalence:\n",
    "Let $S=\\{0000,0010,0110,1100\\}$. $0000\\sim 0110$ because there is a sequence of one bit changes you can make to get from $0000$ to $0110$ in $S$: $0000$, $0010$, $0110$. But $1100$ is not equivalent to any of the other ones. So $\\{0000,0010,0110\\}$ form an equivalence class and $\\{1100\\}$ is the other, thus there are 2 clusters.\n",
    "- To use early stopping with tolerance in agglomerative clustering, each time two closest clusters are about to be merged, the distance between those clusters is compared to the tolerance. If the distance is larger than the tolerance, the clusters merge and the algorithm continues, otherwise, they aren't merged and the algorithm terminates.\n",
    "- To find out which linkage rule to use, think about how you would figure out which equivalence class the string $0001$ belongs to in the previously given example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
       "0   0   0   0   0   0   0   1   1   0   1   1   1   1   1   0   0\n",
       "1   0   0   0   0   0   0   1   1   0   0   1   1   1   1   0   0\n",
       "2   0   0   0   0   0   0   0   1   0   0   1   1   1   1   0   0\n",
       "3   0   0   0   0   0   0   1   1   0   0   1   1   1   1   0   0\n",
       "4   0   0   0   0   0   1   1   1   0   0   1   1   1   1   0   0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "S=pd.read_csv(\"bitstrings.csv\", names = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "S.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Assume that we are in the middle of performing the agglomerative clustering algorithm, so we have some clusters $C_1,C_2, ..., C_m$. How should we decide which two clusters to combine next? Recall that equivalence relations must satisfy the transitive property (if A\\~B and B\\~C, then A\\~C). So if some bit-string $s_1$ in cluster $C_i$ can be transformed to the bit-string $s_2$ in cluster $C_j$ with one bit-flip, because of the transitive property, we know that every bit-string in $C_i$ must also be equivalent to every bit-string in $C_j$. So when computing distances of clusters, we only need to check to see if at least one string in one cluster can be transformed to a string in the other cluster within one bit-flip.\n",
    "\n",
    "Distance metric: We would want to use Manhattan distance for this problem (or Euclidean). The reason is because we want to call two bit-strings equivalent if you can transform one to the other through a series of bit flips. We need to make sure that every string within the sequence is only one bit-flip away from the string before it. So we need a way to count how many bit-flips it takes to transform one string to the other. In this case, Manhattan distance metric will calculate this. (In this specific case, Euclidean distance will also work, because we only need to make sure that one bit is flipped. The Manhattan and Euclidean distances return the same distance of 1 if two strings are one bit-flip apart from each other.)\n",
    "\n",
    "Threshold distance: 1. Using Manhattan distance we know how many bit-flips it takes to change one string to another. But as stated in the Note above, we want to combine clusters if they are one bit-flip apart.\n",
    "\n",
    "Linkage rule: Single-linkage. From the Note above, we need to make sure that at least one string from one cluster can be transformed to another string in a different cluster in 1 bit-flip. This is the same as saying that there is a string in one cluster that is at most 1 distance away from another string in the other cluster. We can check this by simply checking the shortest distance between any pair of strings from the two clusters, this distance needs to be at most 1. This is the same as the single-linkage rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2- [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the clustering using the parameters you picked in part a. How many equivalence classes (clusters) are there? Create a bar graph plotting the number of strings in each cluster. (You may need to increase your distance threshold slightly if you don't want the algorithm to terminate when the distance is equal to the threshold.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296.0, 194.0, 193.0, 187.0, 100.0, 100.0, 99.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 32 artists>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=None, affinity=\"manhattan\", linkage=\"single\", distance_threshold=1.01).fit(S)\n",
    "\n",
    "# Computing size of clusters\n",
    "bins = np.zeros(clustering.n_clusters_)\n",
    "for i in clustering.labels_:\n",
    "    bins[i]+=1\n",
    "\n",
    "#To make it easier to visualize, sorting in descending order\n",
    "bins = sorted(bins, reverse=True)\n",
    "#Print size of clusters\n",
    "print(bins)\n",
    "plt.bar(range(0,clustering.n_clusters_), bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3- [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this clustering, determine if the string $0000001101111100$ is equivalent to the string $1011101101111101$. What about string $1001111011001001$ and string $1001111011000000$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "5\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "#converting to numpy array for easier row searching\n",
    "mat = S.to_numpy()\n",
    "\n",
    "#Finding index of each string\n",
    "s1=np.where((mat==(0,0,0,0,0,0,1,1,0,1,1,1,1,1,0,0)).all(1))[0][0]\n",
    "s2=np.where((mat==(1,0,1,1,1,0,1,1,0,1,1,1,1,1,0,1)).all(1))[0][0]\n",
    "s3=np.where((mat==(1,0,0,1,1,1,1,0,1,1,0,0,1,0,0,1)).all(1))[0][0]\n",
    "s4=np.where((mat==(1,0,0,1,1,1,1,0,1,1,0,0,0,0,0,0)).all(1))[0][0]\n",
    "\n",
    "#printing the cluster label of each string. The labels need to be equal for them to be equivalent strings\n",
    "print(str(clustering.labels_[s1]))\n",
    "print(str(clustering.labels_[s2]))\n",
    "print(str(clustering.labels_[s3]))\n",
    "print(str(clustering.labels_[s4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first two strings are in the same cluster, so they are equivalent to each other. The next pair of strings are in different clusters, and so they aren't equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4- [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo the clustering on the dataset, but this time using both of the other linkage rules (keep everything else the same), and report the number of clusters for both. Is there any difference from your number of clusters in part b? If so, explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618\n",
      "617\n"
     ]
    }
   ],
   "source": [
    "clustering2 = AgglomerativeClustering(n_clusters=None, affinity=\"manhattan\", linkage=\"complete\", distance_threshold=1.01).fit(S)\n",
    "print(clustering2.n_clusters_)\n",
    "\n",
    "clustering3 = AgglomerativeClustering(n_clusters=None, affinity=\"manhattan\", linkage=\"average\", distance_threshold=1.01).fit(S)\n",
    "print(clustering3.n_clusters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of clusters rose drastically. In the single-linkage rule, if the closest pair of strings from two clusters are within 1 distance of each other, the two clusters will merge. This makes it fairly easy for clusters to merge, there just needs to be a pair of strings with one different bit in the two clusters. However, in complete-linkage, two clusters are merged if the furthest pair of points within the two clusters are at most 1 distance away. Because we are now comparing to the furthest pair rather than the shortest, this means every string in one cluster needs to be one bit-flip away from every other string in the other cluster. This is very unlikely to happen, and will force small clusters. So the agglomerative clustering will terminate soon. The same problem happens with average distance. Instead we are only comparing average distance, but this will still mean that multiple strings have to be one bit-flip away from a few other strings in the other cluster. This is also very unlikely and will force the algorithm to terminate quickly with small clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5- [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the dataset again, this time using K-means clustering with the number of clusters set to the number you found in part b. Create a bar plot for the size of the clusters. Compare with your plot in part b, how do these results differ? Give an explanation for this difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.0, 81.0, 78.0, 74.0, 72.0, 68.0, 68.0, 59.0, 58.0, 53.0, 50.0, 49.0, 31.0, 29.0, 26.0, 24.0, 23.0, 22.0, 22.0, 21.0, 20.0, 20.0, 19.0, 19.0, 19.0, 18.0, 18.0, 15.0, 15.0, 14.0, 13.0, 13.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 32 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=clustering.n_clusters_, random_state=0).fit(S)\n",
    "\n",
    "bins2 = np.zeros(clustering.n_clusters_)\n",
    "for i in kmeans.labels_:\n",
    "    bins2[i]+=1\n",
    "\n",
    "bins2 = sorted(bins2, reverse=True)\n",
    "print(bins2)\n",
    "plt.bar(range(0,clustering.n_clusters_), bins2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot less variation in the sizes of the clusters compared to the agglomerative clustering algorithm. The larger clusters are smaller than before, and the smaller clusters are larger than before.\n",
    "Based on the plot from part b, we can see that there are a few large clusters in the dataset, followed by a few random samples that aren't close enough to other strings to create a large equivalence class. So in the KMeans algorithm, we intialize random means. Then as the iterations increase, the means will move closer and closer to the cluster they are trying to capture. But because the structure of our dataset is just a few large clusters with lots of random noise, these means will most likely converge to one of the larger clusters. And there could be multiple means within the same larger cluster, so that's why the larger clusters decreased in size, because they effectively have been split in half because two means converged to that same cluster. Also the means will ignore the random noisy strings, since there aren't enough of them compared to the size of the larger clusters. This explains why the really small clusters no longer exist, since none of the means converged to a small cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
